# ğŸŒŸ LLM Benchmarking Framework ğŸŒŸ

## ğŸ“œ Overview

Welcome to the **LLM Benchmarking Framework**! This project provides a powerful tool for benchmarking **Language Models (LLMs)** across various evaluation metrics including **accuracy**, **hallucination detection**, and **helpfulness evaluation**. Our framework leverages state-of-the-art models like **T5**, **GPT-3/4**, **RoBERTa**, and **BERT**, alongside external fact-checking tools such as **ClaimBuster** to assess the **factual correctness**, **completeness**, and **reliability** of model responses. ğŸ¤–

### ğŸ› ï¸ Key Features
- **ğŸ“ Question Generation**: Automatically generate simple and complex questions from a given prompt using **GPT-3/4** or **T5**.
- **ğŸ” Ground Truth Extraction**: Extract correct answers using **RoBERTa** from reference documents.
- **ğŸ” Hallucination Detection**: Detect hallucinations in responses with **RoBERTa-based fact-checking** or **ClaimBuster**.
- **ğŸ’¬ Helpfulness Evaluation**: Evaluate the helpfulness of responses based on their completeness and relevance using **T5** or **BERT**.

---

## âš™ï¸ Installation

To get started with the framework, follow the steps below to set it up on your local machine:

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/jamuna-veritylabs/benchmarking-framework.git
cd benchmarking-framework
```
2ï¸âƒ£ Install Dependencies
Make sure you have Python 3.7+ installed. Then, install the required libraries with pip:
```bash
pip install -r requirements.txt
```
3ï¸âƒ£ Download Pre-trained Models
Download the necessary pre-trained models for question generation, fact-checking, and helpfulness evaluation:

# Download T5, GPT-3, RoBERTa models
python scripts/download_models.py

### ğŸš€ Usage

Hereâ€™s how to use the framework for generating questions, extracting answers, detecting hallucinations, and evaluating helpfulness:

1ï¸âƒ£ Generate Benchmark Questions
To generate a suite of questions related to a given prompt, run:
python scripts/generate_questions.py --prompt "climate change"

2ï¸âƒ£ Extract Ground Truth
To extract the correct answer for a given question from a context:
python scripts/extract_answer.py --context "Paris is the capital of France." --question "What is the capital of France?"

3ï¸âƒ£ Detect Hallucinations
To check the factual accuracy of a claim:
python scripts/fact_check.py --claim "Global warming causes faster crop growth." --context "Scientific research has shown that global warming has harmful effects on agriculture, such as drought and flooding."

4ï¸âƒ£ Evaluate Helpfulness
To evaluate the helpfulness of a model's response based on an expected answer:
python scripts/evaluate_helpfulness.py --response "Global warming causes unpredictable weather patterns." --expected_answer "Global warming disrupts crop growth patterns and leads to unpredictable weather."

5ï¸âƒ£ Run Full Benchmark Pipeline
To run the entire benchmarking pipeline across all tasks, simply execute:
python scripts/benchmark_pipeline.py
This will evaluate the models and generate performance reports, including:

Honesty: True/False (with hallucination level)
Helpfulness: Full, Partial, or Missing answers

### ğŸ—‚ï¸ Project Structure

Hereâ€™s how the repository is organized:
```bash
/benchmarking-framework
ai-acc-stack/
â”‚
â”œâ”€â”€ src/                                # Core backend components
â”‚   â””â”€â”€ components/
â”‚       â”œâ”€â”€ ingestion/                  # PDF loading, text extraction, chunking
â”‚       â”‚   â”œâ”€â”€ pdf_ingestion.py
â”‚       â”‚   â””â”€â”€ chunking.py
â”‚       â”‚
â”‚       â”œâ”€â”€ embeddings/                 # Embedding models (SBERT, Titan, etc.)
â”‚       â”‚   â”œâ”€â”€ titan_embedder.py
â”‚       â”‚   â””â”€â”€ sbert_embedder.py
â”‚       â”‚
â”‚       â”œâ”€â”€ vectorstore/                # Vector DB wrappers (Chroma, FAISS, OpenSearch)
â”‚       â”‚   â”œâ”€â”€ chroma_wrapper.py
â”‚       â”‚   â”œâ”€â”€ faiss_store.py
â”‚       â”‚   â””â”€â”€ opensearch_store.py
â”‚       â”‚
â”‚       â”œâ”€â”€ llm/                        # LLM model integration (Bedrock, HF, etc.)
â”‚       â”‚   â”œâ”€â”€ bedrock_llm.py
â”‚       â”‚   â””â”€â”€ prompt_templates.py
â”‚       â”‚
â”‚       â””â”€â”€ rag/                        # Full RAG pipelines / orchestration logic
â”‚           â”œâ”€â”€ pipeline.py
â”‚           â””â”€â”€ qa_workflow.py
â”‚
â”œâ”€â”€ examples/                           # Jupyter notebooks and demo flows
â”‚   â”œâ”€â”€ rag_demo.ipynb                  # Full end-to-end RAG notebook
â”‚   â”œâ”€â”€ sbert_local_demo.ipynb         # Local SBERT + Chroma test
â”‚   â””â”€â”€ titan_bedrock_demo.ipynb       # Titan + Bedrock version
â”‚
â”œâ”€â”€ infra/                              # Terraform remote state config
â”‚   â””â”€â”€ remote-state/
â”‚       â”œâ”€â”€ backend.tf
â”‚       â””â”€â”€ s3.tf
â”‚
â”œâ”€â”€ src/blueprints/                     # Terraform blueprints for RAG deployment
â”‚   â”œâ”€â”€ bedrock_assistant.tf
â”‚   â”œâ”€â”€ vector_db.tf
â”‚   â””â”€â”€ s3_ingestion.tf
â”‚
â”œâ”€â”€ scripts/                            # CLI tools and utilities
â”‚   â”œâ”€â”€ docker/
â”‚   â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”‚   â””â”€â”€ entrypoint.sh
â”‚   â””â”€â”€ deploy.sh                       # One-command infra deployment
â”‚
â”œâ”€â”€ .env                                # Environment config (optional)
â”œâ”€â”€ .gitignore
â”œâ”€â”€ requirements.txt                    # Python dependencies
â”œâ”€â”€ docker-compose.yml                  # Multi-service local dev setup
â”œâ”€â”€ README.md
â””â”€â”€ LICENSE
```
### âš¡ Models and Algorithms Used

This project integrates the following models and algorithms:

T5 and GPT-3/4 for question generation.
RoBERTa for answer extraction and hallucination detection.
ClaimBuster for fact-checking and hallucination detection.
BERT for helpfulness evaluation.
These models are fine-tuned on relevant datasets such as SQuAD, FEVER, and ClaimBuster to ensure robust performance on factual correctness, helpfulness, and hallucination detection.

### ğŸ¤ Contributing

We welcome contributions to this project! Feel free to submit issues, suggest features, or open pull requests to improve the framework.

How to contribute:
Fork the repository ğŸ´.
Clone your forked repo to your local machine ğŸ’».
Make changes or additions to the code âœ¨.
Create a pull request to submit your changes.

### ğŸ“œ License

This project is licensed under the MIT License. See the LICENSE file for more details.

### Acknowledgments

T5, GPT-3/4, RoBERTa, BERT for their state-of-the-art capabilities.
ClaimBuster for providing fact-checking tools.
Special thanks to the authors of the FEVER and SQuAD datasets.

### ğŸ“± Contact

For any questions or suggestions, feel free to open an issue or contact me at jamuna@veritylabs.ai
